---
title: Setting Up a User Research Function
subtitle: UX Research, Player Research, Process Design, Pipeline Design, Team Leadership, Facilitation, Communication, Accessibility, Expert Analysis
image: assets/img/portfolio/user-research.png
alt: Trine 4 key art

caption:
  title: Setting Up a User Research Function
  subtitle: UX Research, Player Research, Process Design, Pipeline Design, Team Leadership, Facilitation, Communication, Accessibility, Expert Analysis
  thumbnail: assets/img/portfolio/user-research.png
---
When I joined Frozenbyte, they had an established user research pipeline: Once the game was near shipping and almost completely polished, they would invite players over to their offices to play the game and give feedback on it.

The support team would organise and moderate these sessions. They would take the latest build on the day of the test, record the entire session, and inform the game team that the videos were available for them. The more tests they ran, especially on the same game, the better observations they could make during the sessions, and flag issues directly with the team immediately after the playtest session was over.

Over the years they have learned a lot about player perception and thinking through this process.

Over my tenure at Frozenbyte, I improved the process is multiple ways.

As the moderators could identify some issues during the tests, we created a system for them to document their own findings in the tests and share them with the team. The intention was to speed up the flagging of obvious issues, while the designers would go over the entire footage and look for more subtle design and gameplay related issues.

However, due to time pressures, optimism, and lack of clear boundaries on responsibilities, the designers soon started treating the reports moderators handed them, as enough, as the issues were obvious enough to be seen by everybody.

However, as the moderators were barely interacting with the game teams as their main task was player support, the moderators could never understand design intents or even guess what the designers were looking for.

During this time we also tried to bundle more test sessions into one, as previously each playtest session was analysed in isolation. This time also coincided with the pandemic, so we trialed remote playtest sessions internally.

I understood that the biggest issue blocking us from getting more out of our playtests was the lack of clear research questions. The moderators, and myself, had anticipated one, but often defaulted into “we’ll log everything we see”. As a research question was never required, the teams didn’t know to pose any.

User research had effectively become a checklist item to be crossed off, not something the teams put a lot of conscious effort into.

So, working together with management, designers, artists, writers, and developers, I set out to understand what people wanted to know about our users at any given time.

I trialed several initiatives:
* Concept tests for art and writing internally to see how we could feel more confident in our narrative structure and character concepts
* Launched large-scale surveys for an MMO in beta-testing and shared findings with the entire company with infographics
* Compiled ad hoc data collection efforts into a more concentrated and mindful one for an MMO in beta-testing to help the dev team to understand their players’ actions better and to help them find synergies between data across different features
* Established and documented expert reviews so internal experts could give feedback to dev teams in a more structured manner
* As part of expert reviews, got the rendering team to create colour blindness filters into our engine, so the game teams and QA could play the game in black and white or with different kinds of colour blindnesses to see if contrast between gameplay elements was ever an issue.
* Created a template for user tests, to help game teams identify the research questions each user test could focus on as well as the question’s effect on the entire test. This also helped facilitate communication within and clarify the division of responsibilities between moderators and designers.

These initiatives had several company-wide benefits:
* Increased confidence in design decisions
* Enhanced cross-team and cross-discipline communication
* Actionable insights from player data
* Improved research efficiency
* Streamlined internal processes for feedback